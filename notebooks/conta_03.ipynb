{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992f89bb722c4ec6aae2b4409db1365f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/206766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b508da9c2e0940b191f4a14060eb68e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD AND STURCTURE DATA\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples['sentences'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_data, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/maumaria/code/mmf3358/conta_tu_o_conto/tokenizer_config.json',\n",
       " '/home/maumaria/code/mmf3358/conta_tu_o_conto/special_tokens_map.json',\n",
       " '/home/maumaria/code/mmf3358/conta_tu_o_conto/vocab.json',\n",
       " '/home/maumaria/code/mmf3358/conta_tu_o_conto/merges.txt',\n",
       " '/home/maumaria/code/mmf3358/conta_tu_o_conto/added_tokens.json',\n",
       " '/home/maumaria/code/mmf3358/conta_tu_o_conto/tokenizer.json')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained('/home/maumaria/code/mmf3358/conta_tu_o_conto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FREEZE WEIGHTS\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# LoRa\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# TRAINING\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=500,\n",
    "        learning_rate=1e-3,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs',\n",
    "        auto_find_batch_size=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "torch.save(model.state_dict(), 'lora.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/maumaria/code/mmf3358/conta_tu_o_conto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m trained_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/maumaria/code/mmf3358/conta_tu_o_conto\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "trained_model = load_model('/home/maumaria/code/mmf3358/conta_tu_o_conto')\n",
    "trained_tokenizer = '/home/maumaria/code/mmf3358/conta_tu_o_conto'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The White rabbit went to school, you couldn\\'t find the red rabbit here... We also have to find a new place for the new rabbit!\" â€” Gunga to the rabbit in the cave.[src]\\n\\nGunga and Taki both worked together, and Gunga and Taki started a new school in the cave. In order to learn about the magic that had been bestowed on Kiku, Gunga used one of his own magic spells to cast the secret spell of'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer = trained_tokenizer)\n",
    "generator(\"The White rabbit went to school\", max_length=100, num_return_sequences=5)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "you are a story generator. give me a fairy tale about prince johnny and his love for his wife, and I'll let you know if you like it. I've got a story for you, too. I've got a story for you, too. I\n"
     ]
    }
   ],
   "source": [
    "prompt = \"you are a story generator. give me a fairy tale about prince john\"\n",
    "\n",
    "   \n",
    "prompt = generator(prompt, max_length=40, num_return_sequences=1, max_new_tokens=40, temperature=.6)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"\\n\\n{prompt[0]['generated_text']}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=40) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=40) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=40) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=40) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "the prince john went to the market to buy apples and apples by the pound and said he saw the prince John in the car and said he was going to steal them. The prince John said no, he was going to get his apples by the pound he went to the shop and bought apples, apples and apples by the pound and there was a fight and he broke his neck and went to the hospital and said he went to the hospital to get his he had a good idea of how to get them, and he was going to steal them. He said he was going to steal them if his brother John could help him. The prince John said he he was going to steal apples and apples by the pound. The prince John said he was going to steal apples and apples by the pound. The prince John said he was going to steal apples and apples but he was going to steal the apples. The prince John said he wanted to know what John was doing.\n",
      "\n",
      "The prince John said what John was doing is one of his most basic business transactions\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the prince john went to the market to buy apples\"\n",
    "\n",
    "for i in range(5):\n",
    "    new_prompt = generator(prompt, max_length=40, num_return_sequences=1, max_new_tokens=40, temperature=.6)\n",
    "    prompt = new_prompt[0]['generated_text']\n",
    "    \n",
    "print(f\"\\n\\n\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conta_tu_o_conto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
